{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🤗 Quanto: a pytorch quantization toolkit"
      ],
      "metadata": {
        "id": "f8IjhuqOYKdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to this tutorial where we showcase how to use `quanto` library quantize any model in 8, 4, even 2 bit precision on GPU / CPU and MPS device! Let's get started 🔥"
      ],
      "metadata": {
        "id": "WKhpIxawYLyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "W6tCv-A_bJVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install quanto accelerate"
      ],
      "metadata": {
        "id": "7TqRmAXpbLWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c2de8e7-623d-492f-91d6-4dc4ff177d0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting quanto\n",
            "  Downloading quanto-0.1.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from quanto) (2.2.1+cu121)\n",
            "Collecting ninja (from quanto)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from quanto) (1.25.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from quanto) (0.4.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.2.0->quanto)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.2.0->quanto)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.2.0->quanto)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.2.0->quanto)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.2.0->quanto)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.2.0->quanto)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.2.0->quanto)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.2.0->quanto)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.2.0->quanto)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=2.2.0->quanto)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.2.0->quanto)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->quanto) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.2.0->quanto)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.2.0->quanto) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.2.0->quanto) (1.3.0)\n",
            "Installing collected packages: ninja, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, quanto, accelerate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from quanto import freeze, quantize\n",
        "import torch"
      ],
      "metadata": {
        "id": "p1DlU-6vlxKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# monkey patched for quanto\n",
        "def named_module_tensors(module, recurse=False):\n",
        "    for named_parameter in module.named_parameters(recurse=recurse):\n",
        "      name, val = named_parameter\n",
        "      flag = True\n",
        "      if hasattr(val,\"_data\") or hasattr(val,\"_scale\"):\n",
        "        if hasattr(val,\"_data\"):\n",
        "          yield name + \"._data\", val._data\n",
        "        if hasattr(val,\"_scale\"):\n",
        "          yield name + \"._scale\", val._scale\n",
        "      else:\n",
        "        yield named_parameter\n",
        "\n",
        "    for named_buffer in module.named_buffers(recurse=recurse):\n",
        "      yield named_buffer\n",
        "\n",
        "def dtype_byte_size(dtype):\n",
        "    \"\"\"\n",
        "    Returns the size (in bytes) occupied by one parameter of type `dtype`.\n",
        "    \"\"\"\n",
        "    import re\n",
        "    if dtype == torch.bool:\n",
        "        return 1 / 8\n",
        "    bit_search = re.search(r\"[^\\d](\\d+)$\", str(dtype))\n",
        "    if bit_search is None:\n",
        "        raise ValueError(f\"`dtype` is not a valid dtype: {dtype}.\")\n",
        "    bit_size = int(bit_search.groups()[0])\n",
        "    return bit_size // 8\n",
        "\n",
        "def compute_module_sizes(model):\n",
        "    \"\"\"\n",
        "    Compute the size of each submodule of a given model.\n",
        "    \"\"\"\n",
        "    from collections import defaultdict\n",
        "    module_sizes = defaultdict(int)\n",
        "    for name, tensor in named_module_tensors(model, recurse=True):\n",
        "      size = tensor.numel() * dtype_byte_size(tensor.dtype)\n",
        "      name_parts = name.split(\".\")\n",
        "      for idx in range(len(name_parts) + 1):\n",
        "        module_sizes[\".\".join(name_parts[:idx])] += size\n",
        "\n",
        "    return module_sizes"
      ],
      "metadata": {
        "id": "iv1gmUYcbHz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Without quantization"
      ],
      "metadata": {
        "id": "qqmyZYGbbO8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first load following model `bigscience/bloom-560m` and play with it."
      ],
      "metadata": {
        "id": "azJa4lwzZQgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\", low_cpu_mem_usage=True, torch_dtype = torch.float32).to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")"
      ],
      "metadata": {
        "id": "ycXroVp9oWwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "h5XBo7KwdrJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello my name is\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=10)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "b0Yv0B0_qXqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the model size is 2.2GB (~560m parameters * 4 bytes) since we loaded the model in torch.float32 (32 bits = 4 bytes)."
      ],
      "metadata": {
        "id": "8PXtRPgSaE2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "module_sizes = compute_module_sizes(model.transformer)\n",
        "print(module_sizes)\n",
        "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")"
      ],
      "metadata": {
        "id": "hAuJQ659scbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.transformer.h[0].self_attention.query_key_value.weight)"
      ],
      "metadata": {
        "id": "NenhCQBmxb7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8-bit quantization (weight only)"
      ],
      "metadata": {
        "id": "2BC809CYugOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will use quanto to quantize the model in 8 bits. To do that, we can specify that we want the weights to be quantized with the quanto data type `qint8`. In this example, we will only quantize the weights but we can also quantize the activation."
      ],
      "metadata": {
        "id": "k-o8zAQ8aeKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Quantize\n",
        "At this stage, only the inference of the model is modified to dynamically quantize the weights. (aka dynamic quantization)"
      ],
      "metadata": {
        "id": "4V_amrl-xG9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from quanto import qint8\n",
        "# quantization happens in-place\n",
        "quantize(model.transformer, weights=qint8, activations=None)"
      ],
      "metadata": {
        "id": "duHmUp9_uB4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the `Linear` layers have been replaced by `QLinear` from quanto library. Also, we don't quantize the `lm_head`, so that we don't get performance degradation with LLMs."
      ],
      "metadata": {
        "id": "i0bDec26b57J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "WwhtYmA2zfma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.transformer.h[0].self_attention.query_key_value.weight)"
      ],
      "metadata": {
        "id": "ZBKyTNZCzu9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module_sizes = compute_module_sizes(model.transformer)\n",
        "\n",
        "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")"
      ],
      "metadata": {
        "id": "H9ZPSw6Ad_Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the weights are still on torch.float32 and we quantize the weights only during inference.\n",
        "This intermediate state is useful for:\n",
        "- **Calibration** : Record the activation ranges while passing representative samples through the quantized model\n",
        "- **Quantization-Aware-Training** : If the performance of the model degrades too much, one can tune it for a few epochs to recover the float model performance\n"
      ],
      "metadata": {
        "id": "DlisfZMpxP2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also dynamically quantize the weights by calling the property `qweight`. This is what is done in practice during inference if we don't freeze the weights."
      ],
      "metadata": {
        "id": "BT8PqAtDcPgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.transformer.h[0].self_attention.query_key_value.qweight)"
      ],
      "metadata": {
        "id": "Cg2PKkIuyZS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Freeze the weights (float -> int)"
      ],
      "metadata": {
        "id": "dODA6rR0z297"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When freezing a model, its float weights are replaced by quantized integer weights."
      ],
      "metadata": {
        "id": "9RO-IDrZ0cWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freeze(model)"
      ],
      "metadata": {
        "id": "arRJjXoav6bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that we have integer weights now"
      ],
      "metadata": {
        "id": "EutGRC-30mcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.transformer.h[0].self_attention.query_key_value.weight)"
      ],
      "metadata": {
        "id": "MuUtWPQn0eNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model size is only 1.3 GB now ! Note that the embedding layer takes 1GB and is not quantized. This is why the model size is not divided by 4 as expected if the entire model is converted to from 32 bits to 8 bits. You can see that by doing: `print(module_sizes)`"
      ],
      "metadata": {
        "id": "7vS9vv5MdAbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "module_sizes = compute_module_sizes(model.transformer)\n",
        "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")"
      ],
      "metadata": {
        "id": "5BfIEhc80kZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello my name is\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=10)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "DtBkLR_9Y-l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v8aoaA-dJwr9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}